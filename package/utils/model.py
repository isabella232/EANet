import torch.nn as nn


def init_classifier(m):
    if isinstance(m, nn.Linear):
        nn.init.normal_(m.weight, std=0.001)
        if m.bias is not None:
            nn.init.constant_(m.bias, 0)


def create_embedding(in_dim=None, out_dim=None):
    layers = [
        nn.Linear(in_dim, out_dim),
        # nn.BatchNorm1d(out_dim),
        # nn.ReLU(inplace=True)
    ]
    return nn.Sequential(*layers)
